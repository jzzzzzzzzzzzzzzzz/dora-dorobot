"""
DoRobot Policy implementation for ACT model inference.
"""

import torch
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional

from .utils.device import get_torch_device, is_half_precision_supported
from .utils.preprocessor import DoRobotPreprocessor
from .utils.postprocessor import DoRobotPostprocessor
from .model import ACTModelLoader, SimpleACTModel


class DoRobotPolicy:
    """DoRobotç­–ç•¥ç±»ï¼Œç”¨äºACTæ¨¡å‹æ¨ç†"""
    
    def __init__(self, model_path: str, device: str = "auto", use_real_model: bool = True):
        """
        åˆå§‹åŒ–ç­–ç•¥
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ç±»å‹ ("auto", "cpu", "cuda", "npu")
            use_real_model: æ˜¯å¦ä½¿ç”¨çœŸæ­£çš„ACTæ¨¡å‹
        """
        self.model_path = Path(model_path)
        self.device = get_torch_device(device)
        self.use_real_model = use_real_model
        
        # æ£€æŸ¥åŠç²¾åº¦æ”¯æŒ
        self.use_half_precision = is_half_precision_supported(device)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._load_model()
        self._init_preprocessor()
        self._init_postprocessor()
        
        print(f"âœ… DoRobotç­–ç•¥åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
        if self.use_half_precision:
            print("âœ… å¯ç”¨åŠç²¾åº¦æ¨ç†")
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            if self.use_real_model:
                # ä½¿ç”¨çœŸæ­£çš„ACTæ¨¡å‹
                loader = ACTModelLoader(str(self.model_path), str(self.device))
                self.model = loader.get_model()
                self.config = loader.config
                print("âœ… åŠ è½½çœŸæ­£çš„ACTæ¨¡å‹")
            else:
                # ä½¿ç”¨ç®€åŒ–æ¨¡å‹è¿›è¡Œæµ‹è¯•
                self.model = SimpleACTModel()
                self.model = self.model.to(self.device)
                self.config = {"type": "simple_act"}
                print("âœ… åŠ è½½ç®€åŒ–ACTæ¨¡å‹ç”¨äºæµ‹è¯•")
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°ç®€åŒ–æ¨¡å‹")
            self.model = SimpleACTModel()
            self.model = self.model.to(self.device)
            self.config = {"type": "simple_act"}
            self.use_real_model = False
    
    def _init_preprocessor(self):
        """åˆå§‹åŒ–é¢„å¤„ç†å™¨"""
        self.preprocessor = DoRobotPreprocessor(
            device=self.device,
            use_half_precision=self.use_half_precision
        )
    
    def _init_postprocessor(self):
        """åˆå§‹åŒ–åå¤„ç†å™¨"""
        self.postprocessor = DoRobotPostprocessor(
            model_path=self.model_path,
            device=self.device
        )
    
    def _should_use_half_precision(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨åŠç²¾åº¦"""
        return self.use_half_precision and self.device.type in ["cuda", "npu"]
    
    def predict(self, 
                images: Dict[str, np.ndarray],
                joint_state: np.ndarray,
                return_sequence: bool = False) -> np.ndarray:
        """
        æ‰§è¡Œæ¨ç†é¢„æµ‹
        
        Args:
            images: å›¾åƒå­—å…¸ {"image_top": array, "image_wrist": array}
            joint_state: å…³èŠ‚çŠ¶æ€æ•°ç»„ (6,)
            return_sequence: æ˜¯å¦è¿”å›åŠ¨ä½œåºåˆ—
            
        Returns:
            åŠ¨ä½œæ•°ç»„ (6,) æˆ–åŠ¨ä½œåºåˆ— (n_action_steps, 6)
        """
        with torch.no_grad():
            # é¢„å¤„ç†è¾“å…¥
            processed_inputs = self.preprocessor.process(
                images=images,
                joint_state=joint_state
            )
            
            # æ¨¡å‹æ¨ç†
            if self.use_real_model:
                # çœŸæ­£çš„ACTæ¨¡å‹æ¨ç†
                actions = self.model(
                    images=processed_inputs["images"],
                    state=processed_inputs["state"]
                )  # (B, n_action_steps, 6)
                
                if return_sequence:
                    # è¿”å›æ•´ä¸ªåŠ¨ä½œåºåˆ—
                    actions = actions.squeeze(0)  # (n_action_steps, 6)
                else:
                    # è¿”å›ç¬¬ä¸€ä¸ªåŠ¨ä½œ
                    actions = actions[:, 0, :]  # (B, 6)
                    actions = actions.squeeze(0)  # (6,)
            else:
                # ç®€åŒ–æ¨¡å‹æ¨ç†
                if self.use_real_model:
                    # æ¨¡æ‹ŸçœŸå®æ¨¡å‹çš„è¾“å…¥æ ¼å¼
                    combined_input = torch.cat([
                        processed_inputs["state"],
                        processed_inputs["images"]["image_top"].flatten(1),
                        processed_inputs["images"]["image_wrist"].flatten(1)
                    ], dim=1)
                else:
                    # ç®€åŒ–è¾“å…¥
                    combined_input = processed_inputs["state"]
                
                actions = self.model(combined_input)  # (B, 6)
                actions = actions.squeeze(0)  # (6,)
            
            # åå¤„ç†è¾“å‡º
            processed_actions = self.postprocessor.process(actions)
            
            return processed_actions
    
    def get_action_sequence(self, 
                           images: Dict[str, np.ndarray],
                           joint_state: np.ndarray) -> np.ndarray:
        """
        è·å–åŠ¨ä½œåºåˆ—
        
        Args:
            images: å›¾åƒå­—å…¸
            joint_state: å…³èŠ‚çŠ¶æ€
            
        Returns:
            åŠ¨ä½œåºåˆ— (n_action_steps, 6)
        """
        return self.predict(images, joint_state, return_sequence=True)
    
    def get_single_action(self, 
                         images: Dict[str, np.ndarray],
                         joint_state: np.ndarray) -> np.ndarray:
        """
        è·å–å•ä¸ªåŠ¨ä½œ
        
        Args:
            images: å›¾åƒå­—å…¸
            joint_state: å…³èŠ‚çŠ¶æ€
            
        Returns:
            å•ä¸ªåŠ¨ä½œ (6,)
        """
        return self.predict(images, joint_state, return_sequence=False)
